{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f7e49a07-25d8-4399-b6e2-b8309803f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import pathlib\n",
    "from typing import List, Sequence, Optional, Set, Tuple\n",
    "import typing as tp\n",
    "import attr\n",
    "import nptyping as npt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scann\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e24563ed-ef42-4fbe-bef7-a62e8e17b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tp.TYPE_CHECKING:\n",
    "    import os  # noqa\n",
    "\n",
    "\n",
    "PathLike = tp.Union[str, \"os.PathLike[str]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "34a26177-489c-489b-a650-39b24cbf0d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directional.txt\t\t\t    LICENSE    README.md  wiki-100k.txt\n",
      "google-10000-english-no-swears.txt  notebooks  src\t  wordlist-eng.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f473f9-d923-4798-85f3-3c730715429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(list_of_tokens: List[str]) -> List[str]:\n",
    "    return [token.strip().upper() for token in list_of_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0845d186-2381-4440-8ffb-d41b36a95e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordList:\n",
    "    def __init__(\n",
    "            self,\n",
    "            wordlist_path: str,\n",
    "            illegals_paths: Optional[List[str]] = None,\n",
    "            allowed_paths: Optional[List[str]] = None\n",
    "    ):\n",
    "        path = pathlib.Path(wordlist_path)\n",
    "        with path.open() as f:\n",
    "            self.words = regularize(f.read().splitlines())\n",
    "        self.illegals = self.load_texts(illegals_paths) if illegals_paths else set()\n",
    "        self.allowed = self.load_texts(allowed_paths) if allowed_paths else set()\n",
    "        # If it is illegal for the board, it will be detected later on\n",
    "        self.allowed.update(self.words)\n",
    "\n",
    "    def load_texts(self, paths: tp.List[PathLike]) -> tp.Set[str]:\n",
    "        texts = set()\n",
    "        for pth in paths:\n",
    "            path = pathlib.Path(pth)\n",
    "            with path.open() as f:\n",
    "                texts.update(regularize(f.read().splitlines()))\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "109eb7f1-7f16-44e1-aadb-5dc9c06645ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path(\"../../wordlist-eng.txt\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "312ae070-32b1-4ce5-aace-0fc936b3cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = WordList(\n",
    "    \"../../wordlist-eng.txt\",\n",
    "    [\"../../directional.txt\"],\n",
    "    [\"../../wiki-100k.txt\", \"../../google-10000-english-no-swears.txt\", \"../../custom_whitelist.txt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49388510-fb9b-4c8c-ae4c-2d6bc4674a69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.WordList at 0x7fdd4c2521f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "15dd6b9a-e4b2-4b76-98ae-3b7ea81f70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175d2d1c-0b26-4d7a-9e2e-6841ce016b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BOTTOM',\n",
       " 'DOWN',\n",
       " 'EAST',\n",
       " 'LEFT',\n",
       " 'NORTH',\n",
       " 'RIGHT',\n",
       " 'SOUTH',\n",
       " 'TOP',\n",
       " 'UP',\n",
       " 'WEST'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist.illegals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "920ca30f-1344-4db2-847e-a146aaa6b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LEAVES_TO_SEARCH = 300\n",
    "PRE_REORDER_NUM_NEIGHBOURS = 250\n",
    "\n",
    "\n",
    "labels = [\"BLUE\"] * 9 + [\"RED\"] * 8 + [\"BYSTANDER\"] * 7 + [\"ASSASSIN\"]\n",
    "bot_labels = np.array([\"OURS\", \"THEIRS\", \"BYSTANDER\", \"ASSASSIN\"])\n",
    "valid_teams = {\"BLUE\", \"RED\"}\n",
    "unique_labels = np.unique(labels).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0d500a-8ace-4bb9-ba5d-6f0e638b68ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2cfea306-35c0-4dcb-9ae3-d1a3dfa2fa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_superstring_or_substring(word: str, target: str) -> bool:\n",
    "    return target in word or word in target\n",
    "\n",
    "\n",
    "class Board:\n",
    "    def __init__(self, wordlist: WordList, rng: tp.Optional[object] = None) -> None:\n",
    "        self.wordlist = wordlist\n",
    "        if rng is None:\n",
    "            rng = default_rng\n",
    "        self.rng = np.random.default_rng(rng)\n",
    "        self.words = self.rng.choice(wordlist.words, 25, replace=False)\n",
    "        self.word2index = {word: i for i, word in enumerate(self.words)}\n",
    "        self.labels: tp.List[str] = self.rng.permutation(labels)\n",
    "        self.reset_game()\n",
    "\n",
    "    def is_related_word(self, word: str) -> bool:\n",
    "        word = word.upper()\n",
    "        return any(is_superstring_or_substring(word, target) for target in self.words)\n",
    "\n",
    "    def is_illegal(self, word: str) -> bool:\n",
    "        word = word.upper()\n",
    "        return (\n",
    "            self.is_related_word(word)\n",
    "            or word in self.wordlist.illegals\n",
    "            or word not in self.wordlist.allowed\n",
    "        )\n",
    "\n",
    "    def batch_is_illegal(self, words: npt.NDArray[npt.Shape[\"*\"], npt.typing_.Str]) -> npt.NDArray[npt.Shape[\"*\"], npt.typing_.Bool]:\n",
    "        return np.array([self.is_illegal(w) for w in words])\n",
    "\n",
    "    def reset_game(self) -> None:\n",
    "        self.chosen = np.array([False] * 25)\n",
    "        self.which_team_guessing = \"BLUE\"\n",
    "        # self.hint_history = []\n",
    "        # self.state_history = None\n",
    "\n",
    "    def opponent_of(self, team: str):\n",
    "        assert team in valid_teams\n",
    "        return list(valid_teams.difference([team]))[0]\n",
    "\n",
    "    def end_turn(self):\n",
    "        self.which_team_guessing = self.opponent_of(self.which_team_guessing)\n",
    "\n",
    "    def is_chosen_with_index(self, word: str) -> tp.Tuple[bool, int]:\n",
    "        if word.upper() not in self.words:\n",
    "            raise KeyError(f\"Word '{word}' is not on the board.\")\n",
    "        index = self.word2index[word]\n",
    "        return self.chosen[index], index\n",
    "\n",
    "    def is_chosen(self, word: str) -> bool:\n",
    "        self.is_chosen_with_index(word)[0]\n",
    "\n",
    "    def choose_word(self, word: str) -> str:\n",
    "        chosen, index = self.is_chosen_with_index(word)\n",
    "        if chosen:\n",
    "            raise ValueError(f\"Word '{word}' has already been chosen!\")\n",
    "        self.chosen[index] = True\n",
    "        return self.labels[index]\n",
    "\n",
    "    def words_that_are_label(self, label):\n",
    "        return self.words[self.labels == label]\n",
    "\n",
    "    @property\n",
    "    def blue_words(self):\n",
    "        return self.words_that_are_label(\"BLUE\")\n",
    "\n",
    "    @property\n",
    "    def red_words(self):\n",
    "        return self.words_that_are_label(\"RED\")\n",
    "\n",
    "    @property\n",
    "    def bystander_words(self):\n",
    "        return self.words_that_are_label(\"BYSTANDER\")\n",
    "\n",
    "    @property\n",
    "    def assassin_words(self):\n",
    "        \"\"\"There is only one assassin in a regular game, but for the sake of generality, here we go!\"\"\"\n",
    "        return self.words_that_are_label(\"ASSASSIN\")\n",
    "\n",
    "    def indices_for_label(self, label):\n",
    "        return np.where(self.labels == label)[0]\n",
    "\n",
    "    @property\n",
    "    def blue_indices(self):\n",
    "        return self.indices_for_label(\"BLUE\")\n",
    "\n",
    "    @property\n",
    "    def red_indices(self):\n",
    "        return self.indices_for_label(\"RED\")\n",
    "\n",
    "    @property\n",
    "    def bystander_indices(self):\n",
    "        return self.indices_for_label(\"BYSTANDER\")\n",
    "\n",
    "    @property\n",
    "    def assassin_indices(self):\n",
    "        \"\"\"There is only one assassin in a regular game, but for the sake of generality, here we go!\"\"\"\n",
    "        return self.indices_for_label(\"ASSASSIN\")\n",
    "\n",
    "    def jump_to_random_state(self) -> None:\n",
    "        \"\"\"Jump to a valid random state before the end of the game.\n",
    "\n",
    "        There must be 1 assassin, 1-9 blue words, 1-8 red words, and 0-7 bystanders.\n",
    "        Thus, 0-8 blue, 0-7 red and 1-6 bystander words are chosen.\n",
    "        \"\"\"\n",
    "        self.reset_game()\n",
    "        num_blue = self.rng.integers(0, 9)\n",
    "        num_red = self.rng.integers(0, 8)\n",
    "        num_bystanders = self.rng.integers(0, 7)\n",
    "        chosen_blue = self.rng.choice(self.blue_indices, num_blue, replace=False)\n",
    "        chosen_red = self.rng.choice(self.red_indices, num_red, replace=False)\n",
    "        chosen_bystanders = self.rng.choice(\n",
    "            self.bystander_indices, num_bystanders, replace=False\n",
    "        )\n",
    "        chosen_indices = np.concatenate([chosen_blue, chosen_red, chosen_bystanders])\n",
    "        self.chosen[chosen_indices] = True\n",
    "        self.which_team_guessing = self.rng.choice([\"BLUE\", \"RED\"])\n",
    "\n",
    "    def bag_state(self) -> tp.Dict[str, tp.Set[str]]:\n",
    "        return {\n",
    "            label: set(self.words[(self.labels == label) & ~self.chosen])\n",
    "            for label in unique_labels\n",
    "        }\n",
    "\n",
    "    def remaining_words(self) -> npt.NDArray[npt.Shape[\"*\"], npt.typing_.Str]:\n",
    "        return self.words[~self.chosen]\n",
    "\n",
    "    def remaining_words_for_team(self, team: str) -> int:\n",
    "        return (~self.chosen & (self.labels == team)).sum()\n",
    "\n",
    "    def orient_label(self, my_team, opponent_team, label):\n",
    "        if label == my_team:\n",
    "            return \"OURS\"\n",
    "        elif label == opponent_team:\n",
    "            return \"THEIRS\"\n",
    "        return label\n",
    "\n",
    "    def orient_labels_for_team(self, my_team: str) -> npt.NDArray[npt.Shape[\"*\"], npt.typing_.Str]:\n",
    "        opponent_team = self.opponent_of(my_team)\n",
    "        return np.array(\n",
    "            [self.orient_label(my_team, opponent_team, label) for label in self.labels]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b69a5b7d-9477-4ba8-a27a-3ff3d07037e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.choice([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14e4c4e6-0675-48dc-9dfa-361a8955cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliView:\n",
    "    def __init__(self, board: Board):\n",
    "        self.board = board\n",
    "\n",
    "    def spymaster_words_to_display(self):\n",
    "        words = []\n",
    "        # Arguably more readable than the equivalent list comprehension\n",
    "        for w, l, c in zip(self.board.words, self.board.labels, self.board.chosen):\n",
    "            w += f\"_{l[0]}\"\n",
    "            if c:\n",
    "                w = w.lower()\n",
    "            words.append(w) \n",
    "        return words\n",
    "\n",
    "    def operative_words_to_display(self):\n",
    "        words = []\n",
    "        # Arguably more readable than the equivalent list comprehension\n",
    "        for w, l, c in zip(self.board.words, self.board.labels, self.board.chosen):\n",
    "            if c:\n",
    "                w += f\"_{l[0]}\"\n",
    "                w = w.lower()\n",
    "            words.append(w)\n",
    "        return words\n",
    "\n",
    "    def generic_view(self, words_to_display):\n",
    "        words = words_to_display()\n",
    "        print(np.array(words).reshape(5,5))\n",
    "        print(f\"It is {self.board.which_team_guessing}'s turn.\")\n",
    "\n",
    "    def spymaster_view(self):\n",
    "        self.generic_view(self.spymaster_words_to_display)\n",
    "\n",
    "    def operative_view(self):\n",
    "        self.generic_view(self.operative_words_to_display)\n",
    "\n",
    "    def bag_words_team_view(self):\n",
    "        words = self.board.bag_state()\n",
    "        my_team = self.board.which_team_guessing\n",
    "        other_team = \"RED\" if my_team == \"BLUE\" else \"BLUE\"\n",
    "        words_oriented_perspective = {\n",
    "            \"My team\": [w.lower() for w in words[my_team]],\n",
    "            \"Enemy team\": [w.lower() for w in words[other_team]],\n",
    "            \"Neutral\": [w.lower() for w in words[\"BYSTANDER\"]],\n",
    "            \"Death\": [w.lower() for w in words[\"ASSASSIN\"]]\n",
    "        }\n",
    "        return \"\\n\".join([f\"{k}: {', '.join(v)}\" for k, v in words_oriented_perspective.items()])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ae662c-f986-48ac-840a-df12ce046c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([np.arange(3), np.arange(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3591dd77-6307-45e6-83d2-64c77c9f0293",
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Board(wordlist)\n",
    "view = CliView(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e89990f0-c506-43eb-ad9c-33a980208f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DICE_R' 'GOLD_R' 'heart_r' 'NOTE_B' 'VET_R']\n",
      " ['BOOM_B' 'hospital_b' 'COURT_B' 'SUPERHERO_B' 'WIND_B']\n",
      " ['SOUL_B' 'BELT_B' 'SCREEN_B' 'TRIP_R' 'SNOW_R']\n",
      " ['RULER_B' 'CLOAK_R' 'water_b' 'AGENT_R' 'washer_b']\n",
      " ['snowman_b' 'KANGAROO_B' 'HOLE_A' 'CENTAUR_B' 'PUMPKIN_B']]\n",
      "It is BLUE's turn.\n"
     ]
    }
   ],
   "source": [
    "view.spymaster_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b306abfe-ddf0-4629-8455-6c68e58dedc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My team: court, screen, boom, ruler, pumpkin, soul, wind, belt\n",
      "Enemy team: cloak, trip, dice, gold, vet, snow, agent\n",
      "Neutral: superhero, kangaroo, centaur, note\n",
      "Death: hole\n"
     ]
    }
   ],
   "source": [
    "print(view.bag_words_team_view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c03d9db-0439-4bff-829e-314e4a2f9147",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASSASSIN': {'EGYPT'},\n",
       " 'BLUE': {'EAGLE',\n",
       "  'FIGHTER',\n",
       "  'FLUTE',\n",
       "  'JACK',\n",
       "  'JET',\n",
       "  'MAPLE',\n",
       "  'PASTE',\n",
       "  'PISTOL',\n",
       "  'TOOTH'},\n",
       " 'BYSTANDER': {'DRAFT',\n",
       "  'MILLIONAIRE',\n",
       "  'PILOT',\n",
       "  'POISON',\n",
       "  'SHOP',\n",
       "  'SNOWMAN',\n",
       "  'WEB'},\n",
       " 'RED': {'CIRCLE',\n",
       "  'CONCERT',\n",
       "  'CRASH',\n",
       "  'ROULETTE',\n",
       "  'SNOW',\n",
       "  'STREAM',\n",
       "  'SUIT',\n",
       "  'WAKE'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.bag_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "341fbaf8-c43b-4f46-b757-8243b18f1bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  5,  6,  7,  8, 14, 22, 23, 24])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.blue_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "030d89a5-e5a5-47b7-ba6a-97ed7166f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "board.jump_to_random_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29ebd521-dae9-444d-9c39-e70dbb2545ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DICE' 'GOLD' 'heart_r' 'NOTE' 'VET']\n",
      " ['BOOM' 'hospital_b' 'COURT' 'SUPERHERO' 'WIND']\n",
      " ['SOUL' 'BELT' 'SCREEN' 'TRIP' 'SNOW']\n",
      " ['RULER' 'CLOAK' 'water_b' 'AGENT' 'washer_b']\n",
      " ['snowman_b' 'KANGAROO' 'HOLE' 'CENTAUR' 'PUMPKIN']]\n",
      "It is BLUE's turn.\n"
     ]
    }
   ],
   "source": [
    "view.operative_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c96f9b7c-97b5-4e63-b28d-09687e646ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s(auto_attribs=True)\n",
    "class Hint:\n",
    "    word: str\n",
    "    count: tp.Optional[int]\n",
    "    team: str\n",
    "    num_guessed: int = attr.ib(default=0)\n",
    "    num_guessed_correctly: int = attr.ib(default=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15119992-fad1-4d01-a173-84f037b661b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../../../codenames/dataset/glove': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../codenames/dataset/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b96e2b1-7d72-4bfa-8713-44ad2e4791fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = pathlib.Path(\"../../../codenames/dataset/glove.6B.300d.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "473f62d9-1f9a-41c5-af3e-4d9845f81662",
   "metadata": {},
   "outputs": [],
   "source": [
    "with glove_path.open(\"rb\") as f:\n",
    "    glove_vectors = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e103351a-9cd5-4bb6-9a41-128813c4adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorEngine(metaclass=abc.ABCMeta):\n",
    "    # TODO: add self.vectors, self.tokens, self.token2id here too\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def is_valid_token(self, token):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def tokenize(self, phrase):\n",
    "        pass\n",
    "\n",
    "    def vectorize(self, phrase: tp.Union[str, tp.Sequence[str]]) -> npt.NDArray:\n",
    "        if isinstance(phrase, str):\n",
    "            return self.vectors[self.tokenize(phrase)]\n",
    "        tokens = np.array(standardize_length(self.tokenize(phrase)))\n",
    "        return self.vectors[tokens].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94f40307-2349-4f85-9837-c0d57404abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_norm(vec: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize a batch of vectors\n",
    "    \n",
    "    Args:\n",
    "        vec: (batch, dim)\n",
    "    \"\"\"\n",
    "    return vec / np.linalg.norm(vec, axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "44bdd800-6252-4704-9101-dfa8b54ef246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04656  ,  0.21318  , -0.0074364, ...,  0.0090611, -0.20989  ,\n",
       "         0.053913 ],\n",
       "       [-0.076947 , -0.021211 ,  0.21271  , ...,  0.18351  , -0.29183  ,\n",
       "        -0.046533 ],\n",
       "       [-0.25756  , -0.057132 , -0.6719   , ..., -0.16043  ,  0.046744 ,\n",
       "        -0.070621 ],\n",
       "       ...,\n",
       "       [-0.026489 , -0.12316  ,  0.37794  , ...,  0.24287  , -0.013818 ,\n",
       "        -0.26118  ],\n",
       "       [ 1.1097   , -0.033433 , -0.30873  , ...,  0.18013  ,  0.25812  ,\n",
       "        -0.35347  ],\n",
       "       [ 0.13382  , -0.21265  , -0.71983  , ...,  0.46108  ,  0.49777  ,\n",
       "        -0.5036   ]], dtype=float32)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors[np.array([x in wordlist.allowed for x in glove.tokens])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "bf6d0fcd-47f8-48be-b6d8-dd48f16ba41a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NumpyVectorEngine(TextVectorEngine):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vectors: npt.NDArray[npt.Shape[\"*, *\"], npt.typing_.Float],\n",
    "        tokens: npt.NDArray[npt.Shape[\"*\"], npt.typing_.Str],\n",
    "        normalized: bool = False,\n",
    "        use_approximate: bool = True,\n",
    "    ):\n",
    "        self.vectors = vectors\n",
    "        self.tokens = tokens\n",
    "        self.normalized = normalized\n",
    "        self.use_approximate = use_approximate\n",
    "        if self.normalized:\n",
    "            self.vectors = batched_norm(self.vectors)\n",
    "            if self.use_approximate:\n",
    "                self.searcher = (\n",
    "                    scann.scann_ops_pybind.builder(self.vectors, 20, \"dot_product\")\n",
    "                    .tree(\n",
    "                        num_leaves=2000,\n",
    "                        num_leaves_to_search=NUM_LEAVES_TO_SEARCH,\n",
    "                        training_sample_size=250000,\n",
    "                    )\n",
    "                    .score_ah(2, anisotropic_quantization_threshold=0.2)\n",
    "                    .reorder(PRE_REORDER_NUM_NEIGHBOURS)\n",
    "                    .build()\n",
    "                )\n",
    "        self.token2id = {t: i for i, t in enumerate(self.tokens)}\n",
    "\n",
    "    def is_valid_token(self, token: str) -> bool:\n",
    "        return token.strip().upper() in self.token2id\n",
    "\n",
    "    def is_tokenizable(self, phrase: str) -> bool:\n",
    "        return all(token is not None for token in self.tokenize(phrase))\n",
    "\n",
    "    def tokenize(self, phrase):\n",
    "        \"\"\"Simple one-word tokenization. Ignores punctuation.\"\"\"\n",
    "        if isinstance(phrase, str):\n",
    "            phrase = phrase.strip().upper().split()\n",
    "            return [\n",
    "                self.token2id[x] if self.is_valid_token(x) else None for x in phrase\n",
    "            ]\n",
    "        else:\n",
    "            phrase = regularize(phrase)\n",
    "            return [self.tokenize(token) for token in phrase]\n",
    "\n",
    "    def calculate_similarity_to_word_vector(\n",
    "        self, word_vector: npt.NDArray\n",
    "    ) -> npt.NDArray:\n",
    "        if self.normalized:\n",
    "            # assert np.allclose(word_vector.sum(-1), np.ones(word_vector.shape[0]))\n",
    "            return word_vector @ self.vectors.T\n",
    "        else:\n",
    "            return batched_cosine_similarity(word_vector, self.vectors)\n",
    "\n",
    "\n",
    "class Glove(NumpyVectorEngine):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        glove_vector_path: PathLike,\n",
    "        glove_tokens_path: PathLike,\n",
    "        normalized: bool = False,\n",
    "        use_approximate: bool = True,\n",
    "        wordlist: tp.Optional[WordList] = None,\n",
    "    ):\n",
    "        gv_path = pathlib.Path(glove_vector_path)\n",
    "        gt_path = pathlib.Path(glove_tokens_path)\n",
    "        assert gv_path.exists()\n",
    "        assert gt_path.exists()\n",
    "        with gv_path.open(\"rb\") as f:\n",
    "            vectors = np.load(gv_path)\n",
    "        with gt_path.open() as f:\n",
    "            tokens = f.read().splitlines()\n",
    "            tokens = np.array(regularize(tokens))\n",
    "\n",
    "        if wordlist is not None:\n",
    "            allowed_indices = np.array([x in wordlist.allowed for x in glove.tokens])\n",
    "            vectors = vectors[allowed_indices]\n",
    "            tokens = tokens[allowed_indices]\n",
    "\n",
    "        super().__init__(vectors, tokens, normalized=normalized, use_approximate=use_approximate)\n",
    "\n",
    "\n",
    "def prefix_to_word_stub(prefix: str):\n",
    "    word_stub = prefix.split('/c/en/', 1)[1]\n",
    "    word_stub = \" \".join(word_stub.upper().split(\"_\"))\n",
    "    return word_stub\n",
    "\n",
    "\n",
    "def is_valid_prefix(prefix: str, wordlist: WordList) -> bool:\n",
    "    if not prefix.startswith('/c/en/'):\n",
    "        return False\n",
    "    if prefix.startswith('/c/en/#'):\n",
    "        return False\n",
    "    word_stub = prefix_to_word_stub(prefix)\n",
    "    return word_stub in wordlist.allowed\n",
    "\n",
    "\n",
    "class ConceptNet(NumpyVectorEngine):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        conceptnet_hd5_path: PathLike,\n",
    "        wordlist: WordList,\n",
    "        normalized: bool = False,\n",
    "        use_approximate: bool = True,\n",
    "    ):\n",
    "        cn_path = pathlib.Path(conceptnet_hd5_path)\n",
    "        assert cn_path.is_file()\n",
    "        conceptnet = pd.read_hdf(cn_path)\n",
    "\n",
    "        valid_prefixes = [p for p in conceptnet.index if is_valid_prefix(p, wordlist)]\n",
    "        vectors = conceptnet.loc[valid_prefixes].values\n",
    "        tokens = np.array([prefix_to_word_stub(p) for p in valid_prefixes if is_valid_prefix(p, wordlist)])\n",
    "\n",
    "        super().__init__(vectors, tokens, normalized=normalized, use_approximate=use_approximate)\n",
    "\n",
    "    def tokenize(self, phrase):\n",
    "        \"\"\"Basic phrase tokenization. Assumes everything is part of the same phrase. Ignores punctuation.\"\"\"\n",
    "        if isinstance(phrase, str):\n",
    "            phrase = phrase.strip().upper()\n",
    "            if self.is_valid_token(phrase):\n",
    "                return [self.token2id[phrase]]\n",
    "            phrase_arr = phrase.split()\n",
    "            return [\n",
    "                self.token2id[x] if self.is_valid_token(x) else None for x in phrase_arr\n",
    "            ]\n",
    "        else:\n",
    "            phrase = regularize(phrase)\n",
    "            return [self.tokenize(token) for token in phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "963b7f2c-4a99-4ff5-afc1-9906b10a598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Take the batched cosine similarity.\"\"\"\n",
    "    a_norm = batched_norm(a)  # (batch1, dim)\n",
    "    b_norm = batched_norm(b)  # (batch2, dim)\n",
    "    return a_norm @ b_norm.T  # (batch1, batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3aadce40-755c-4ead-80f6-5bf77fdfa659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Board\n",
       "\u001b[0;31mString form:\u001b[0m <__main__.Board object at 0x7fdd33cc6be0>\n",
       "\u001b[0;31mDocstring:\u001b[0m   <no docstring>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a5c551ec-a13c-410a-a566-f146666538f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_x_in_y(x: npt.NDArray, y: npt.NDArray) -> npt.NDArray:\n",
    "    # https://stackoverflow.com/a/8251757\n",
    "    x = np.array(x)  # Allow array-based indexing\n",
    "    index = np.argsort(x)\n",
    "    sorted_x = x[index]\n",
    "    sorted_index = np.searchsorted(sorted_x, y)\n",
    "    y_index = np.take(index, sorted_index, mode=\"clip\")\n",
    "    return y_index[x[y_index] == y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "829d907f-f6f6-4d52-9183-a7b8a8cd778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_length(ragged_matrix: tp.Sequence[tp.Sequence]) -> tp.List[tp.Sequence]:\n",
    "    \"\"\"Standardize the length of a ragged matrix.\n",
    "\n",
    "    Example:\n",
    "        [[3], [4, 5]] -> [[3, 3], [4, 5]]\n",
    "        [[3], [4, 5], [6, 7, 8]] -> [[3, 3, 3, 3, 3, 3], [4, 5, 4, 5, 4, 5], [6, 7, 8, 6, 7, 8]]\n",
    "    \"\"\"\n",
    "    lengths = [len(i) for i in ragged_matrix]\n",
    "    lcm = np.lcm.reduce(lengths)\n",
    "    duplication_count = lcm // lengths\n",
    "    return [row * n_rep for row, n_rep in zip(ragged_matrix, duplication_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "4c5838c4-dd82-4c37-966b-c05fd8465d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GuessStrategyLookup = tp.Dict[\n",
    "    str,\n",
    "    tp.Callable[[npt.NDArray, npt.NDArray, int], tp.Tuple[npt.NDArray, npt.NDArray]],\n",
    "]\n",
    "\n",
    "\n",
    "class GloveGuesser:\n",
    "    def __init__(self, glove: Glove, board: Board, limit: int = 10, p_threshold: float = 0.05):\n",
    "        self.glove = glove\n",
    "        self.board = board\n",
    "        self.limit = limit\n",
    "        self.p_threshold = p_threshold\n",
    "        self.word_suggestion_strategy_lookup = {\n",
    "            \"mean\": self.generate_word_suggestions_mean,\n",
    "            \"minimax\": self.generate_word_suggestions_minimax,\n",
    "            \"approx_mean\": self.generate_word_suggestions_mean_approx,\n",
    "            \"approx_minimax\": self.generate_word_suggestions_minimax_approx,\n",
    "        }\n",
    "        self.guess_strategy_lookup: GuessStrategyLookup = {\n",
    "            \"greedy\": self.guess_greedy,\n",
    "            \"softmax\": self.guess_softmax,\n",
    "        }\n",
    "        self.board_vectors = self.glove.vectorize(self.board.words)\n",
    "\n",
    "    def indices_illegal_words(self, chosen_words: npt.NDArray):\n",
    "        return self.board.batch_is_illegal(chosen_words)\n",
    "\n",
    "    def generate_word_suggestions_abstract(\n",
    "        self,\n",
    "        get_similarity_scores: tp.Callable[[tp.List[str]], npt.NDArray],\n",
    "        words: tp.List[str],\n",
    "        limit: int = 10,\n",
    "    ) -> tp.Tuple[tp.Sequence[str], tp.Sequence[float]]:\n",
    "        for word in words:\n",
    "            if not self.glove.is_tokenizable(word):\n",
    "                raise ValueError(f\"Hint {word} is not a valid hint word!\")\n",
    "        similarity_scores = get_similarity_scores(words)\n",
    "        indices = np.argpartition(-similarity_scores, limit)[:limit]\n",
    "        chosen_words = self.glove.tokens[indices]\n",
    "        similarity_scores = similarity_scores[indices]\n",
    "        return chosen_words, similarity_scores\n",
    "\n",
    "    def get_similarity_scores_mean(self, words: tp.List[str]) -> npt.NDArray:\n",
    "        word_vector = self.glove.vectorize(words).mean(0)[None, :]\n",
    "        similarity_scores = self.glove.calculate_similarity_to_word_vector(word_vector)[\n",
    "            0\n",
    "        ]\n",
    "        return similarity_scores\n",
    "\n",
    "    def generate_word_suggestions_mean(\n",
    "        self, words: tp.List[str], limit: int = 10\n",
    "    ) -> tp.Tuple[tp.Sequence[str], tp.Sequence[float]]:\n",
    "        return self.generate_word_suggestions_abstract(\n",
    "            self.get_similarity_scores_mean, words, limit\n",
    "        )\n",
    "\n",
    "    def get_similarity_scores_minimax(self, words: tp.List[str]) -> npt.NDArray:\n",
    "        word_vector = self.glove.vectorize(words)\n",
    "        similarity_scores = self.glove.calculate_similarity_to_word_vector(\n",
    "            word_vector\n",
    "        ).min(axis=0)\n",
    "        return similarity_scores\n",
    "\n",
    "    def generate_word_suggestions_minimax(\n",
    "        self, words: tp.List[str], limit: int = 10\n",
    "    ) -> tp.Tuple[tp.Sequence[str], tp.Sequence[float]]:\n",
    "        return self.generate_word_suggestions_abstract(\n",
    "            self.get_similarity_scores_minimax, words, limit\n",
    "        )\n",
    "\n",
    "    def generate_word_suggestions_mean_approx(\n",
    "        self, words: tp.List[str], limit: int = 10\n",
    "    ) -> tp.Tuple[tp.Sequence[str], tp.Sequence[float]]:\n",
    "        assert self.glove.normalized and self.glove.use_approximate\n",
    "        word_vector = self.glove.vectorize(words).mean(0)\n",
    "        chosen_words, similarity_scores = self.glove.searcher.search(\n",
    "            word_vector, final_num_neighbors=limit\n",
    "        )\n",
    "        chosen_words = self.glove.tokens[chosen_words]\n",
    "        return chosen_words, similarity_scores\n",
    "\n",
    "    def generate_word_suggestions_minimax_approx(\n",
    "        self, words: tp.List[str], limit: int = 10\n",
    "    ) -> tp.Tuple[tp.Sequence[str], tp.Sequence[float]]:\n",
    "        assert self.glove.normalized and self.glove.use_approximate\n",
    "        word_vectors = self.glove.vectorize(words)\n",
    "        chosen_words = np.unique(\n",
    "            self.glove.searcher.search_batched(word_vectors, final_num_neighbors=limit)[\n",
    "                0\n",
    "            ]\n",
    "        )\n",
    "        similarity_scores = np.min(\n",
    "            word_vectors @ self.glove.vectors[chosen_words].T, axis=0\n",
    "        )\n",
    "        indices = np.argpartition(-similarity_scores, limit)[:limit]\n",
    "        chosen_words = chosen_words[indices]\n",
    "        chosen_words = self.glove.tokens[chosen_words]\n",
    "        similarity_scores = similarity_scores[indices]\n",
    "        return chosen_words, similarity_scores\n",
    "\n",
    "    def filter_words(\n",
    "        self,\n",
    "        chosen_words: npt.NDArray[npt.Shape[\"*\"], npt.typing_.Str],\n",
    "        similarity_scores: npt.NDArray[npt.Shape[\"*\"], npt.typing_.Float],\n",
    "        similarity_threshold=0.0,\n",
    "    ):\n",
    "        words_to_filter = self.indices_illegal_words(chosen_words) | (\n",
    "            similarity_scores < similarity_threshold\n",
    "        )\n",
    "        return chosen_words[~words_to_filter], similarity_scores[~words_to_filter]\n",
    "\n",
    "    def re_rank(\n",
    "        self,\n",
    "        chosen_words: npt.NDArray[npt.Shape[\"*\"], npt.typing_.Str],\n",
    "        similarity_scores: npt.NDArray[npt.Shape[\"*\"], npt.typing_.Float],\n",
    "        limit: int,\n",
    "    ):\n",
    "        indices = np.argsort(-similarity_scores)\n",
    "        chosen_words = chosen_words[indices][:limit]\n",
    "        similarity_scores = similarity_scores[indices][:limit]\n",
    "        return chosen_words, similarity_scores\n",
    "\n",
    "    def give_hint_candidates(\n",
    "        self, targets: tp.List[str], similarity_threshold=0.0, strategy: str = \"minimax\"\n",
    "    ):\n",
    "        generate_word_suggestions = self.word_suggestion_strategy_lookup[strategy]\n",
    "        chosen_words, similarity_scores = generate_word_suggestions(\n",
    "            targets, self.limit * 2\n",
    "        )\n",
    "\n",
    "        chosen_words, similarity_scores = self.filter_words(\n",
    "            chosen_words, similarity_scores\n",
    "        )\n",
    "\n",
    "        return self.re_rank(chosen_words, similarity_scores, self.limit)\n",
    "\n",
    "    def give_hint(\n",
    "        self, targets: tp.List[str], similarity_threshold=0.0, strategy: str = \"minimax\"\n",
    "    ):\n",
    "        \"\"\"Greedily choose the best hint.\"\"\"\n",
    "        chosen_words, _ = self.give_hint_candidates(\n",
    "            targets, similarity_threshold, strategy\n",
    "        )\n",
    "        return chosen_words[0]\n",
    "\n",
    "    def choose_hint_parameters(self, hint: Hint) -> tp.Tuple[str, int]:\n",
    "        \"\"\"TODO: Add strategy mixins\"\"\"\n",
    "        num_words_remaining = self.board.remaining_words_for_team(\n",
    "            self.board.which_team_guessing\n",
    "        )\n",
    "        if hint.count is None:\n",
    "            limit = num_words_remaining\n",
    "        else:\n",
    "            limit = min(hint.count - hint.num_guessed_correctly, num_words_remaining)\n",
    "        return hint.word, limit\n",
    "\n",
    "    def remaining_word_vectors(self) -> npt.NDArray:\n",
    "        return self.board_vectors[~self.board.chosen]\n",
    "\n",
    "    def guess_greedy(\n",
    "        self, remaining_words: npt.NDArray, similarity_scores: npt.NDArray, limit: int\n",
    "    ) -> tp.Tuple[npt.NDArray, npt.NDArray]:\n",
    "        indices = np.argsort(-similarity_scores)\n",
    "        chosen_words = remaining_words[indices][:limit]\n",
    "        similarity_scores = similarity_scores[indices][:limit]\n",
    "        return chosen_words, similarity_scores\n",
    "\n",
    "    def guess_softmax(\n",
    "        self,\n",
    "        remaining_words: npt.NDArray,\n",
    "        similarity_scores: npt.NDArray,\n",
    "        limit: int,\n",
    "        temperature: float = 0.05,\n",
    "    ) -> tp.Tuple[npt.NDArray, npt.NDArray]:\n",
    "        chosen_words = np.random.choice(\n",
    "            remaining_words,\n",
    "            limit,\n",
    "            p=softmax(similarity_scores / temperature),\n",
    "            replace=False,\n",
    "        )\n",
    "        chosen_words_indices = find_x_in_y(remaining_words, chosen_words)\n",
    "        return chosen_words, similarity_scores[chosen_words_indices]\n",
    "\n",
    "    def guess(self, hint: Hint, strategy: str = \"softmax\") -> tp.Sequence[str]:\n",
    "        word, limit = self.choose_hint_parameters(hint)\n",
    "        if not self.glove.is_valid_token(word):\n",
    "            raise ValueError(f\"Hint {word} is not a valid hint word!\")\n",
    "        word_vector = self.glove.vectorize(word)\n",
    "        remaining_words = self.board.remaining_words()\n",
    "        remaining_word_vectors = self.remaining_word_vectors()\n",
    "        similarity_scores = batched_cosine_similarity(\n",
    "            word_vector, remaining_word_vectors\n",
    "        )[0]\n",
    "        guess_with_strategy = self.guess_strategy_lookup[strategy]\n",
    "\n",
    "        chosen_words, similarity_scores = guess_with_strategy(remaining_words, similarity_scores, limit)\n",
    "        indices_above_p_threshold = similarity_scores >= self.p_threshold\n",
    "        return chosen_words[indices_above_p_threshold], similarity_scores[indices_above_p_threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ce4835d-f1b6-49df-a3e2-ddd8d2005550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(3).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ab08e197-50f7-4c90-90c8-9554f502b012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING external/com_google_protobuf/src/google/protobuf/text_format.cc:339] Warning parsing text-format research_scann.ScannConfig: 43:11: text format contains deprecated field \"min_cluster_size\"\n",
      "2022-07-16 05:43:16.633879: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 53044\n",
      "2022-07-16 05:43:18.101845: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 1.467920834s.\n"
     ]
    }
   ],
   "source": [
    "glove = Glove(\"../../../codenames/dataset/glove.6B.300d.npy\", \"../../../codenames/dataset/words\", wordlist=wordlist, normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8854256-b2c4-4eba-aed3-e4293f016f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00898599,  0.04114335, -0.00143521, ...,  0.00174878,\n",
       "        -0.04050838,  0.01040511],\n",
       "       [-0.05874773, -0.05917099,  0.03029284, ..., -0.05357432,\n",
       "        -0.02812364,  0.08165886],\n",
       "       [-0.02817159,  0.0030574 ,  0.0231178 , ..., -0.07676922,\n",
       "        -0.00502329,  0.03069513],\n",
       "       ...,\n",
       "       [ 0.01319627, -0.00705923,  0.03197411, ...,  0.03806217,\n",
       "         0.05397341,  0.0762725 ],\n",
       "       [ 0.13020873, -0.05790341,  0.04985439, ...,  0.0120673 ,\n",
       "         0.04541344, -0.02807007],\n",
       "       [ 0.12707426, -0.08790484,  0.04444436, ...,  0.08578877,\n",
       "         0.09657492, -0.01748439]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors / np.linalg.norm(glove.vectors, axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6cbc833d-b137-4b6d-9350-1c9a664d1d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(glove.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cb37b25-6e15-4f7f-bd4b-18e9db463f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectorize(\"Test\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "81dba50a-3ea5-4a3e-975b-38141c727933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING external/com_google_protobuf/src/google/protobuf/text_format.cc:339] Warning parsing text-format research_scann.ScannConfig: 43:11: text format contains deprecated field \"min_cluster_size\"\n",
      "2022-07-16 04:18:46.087522: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 47240\n",
      "2022-07-16 04:18:47.397724: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:88] PartitionerFactory ran in 1.309583514s.\n"
     ]
    }
   ],
   "source": [
    "conceptnet = ConceptNet(\n",
    "    '../../../conceptnet/mini.h5',\n",
    "    wordlist,\n",
    "    normalized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0126e11c-34ea-4e24-8e9d-1fe7f18e3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in wordlist.words:\n",
    "    assert conceptnet.is_valid_token(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6664a291-a23b-49d4-9c61-189e76494d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.is_illegal(\"AMERICA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "8bc9784c-b87d-462a-90dc-516ec2823369",
   "metadata": {},
   "outputs": [],
   "source": [
    "guesser = GloveGuesser(glove, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "37baf962-a8aa-45f5-a4c9-1e137f3e86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_guesser = GloveGuesser(conceptnet, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "6d8e964b-3f97-47c7-8fae-1ec26d1bbcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[419],\n",
       " [18417],\n",
       " [439],\n",
       " [3304],\n",
       " [2391],\n",
       " [1975],\n",
       " [14862, 18649],\n",
       " [20749],\n",
       " [5935],\n",
       " [2354],\n",
       " [7257],\n",
       " [1237],\n",
       " [530],\n",
       " [2348],\n",
       " [2235],\n",
       " [493],\n",
       " [7756],\n",
       " [6560],\n",
       " [4054],\n",
       " [3835],\n",
       " [2126],\n",
       " [832],\n",
       " [2524],\n",
       " [20175],\n",
       " [2726]]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.tokenize(board.remaining_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6fca3668-d959-48f4-a706-795ecfdd41d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASSASSIN': {'CLUB'},\n",
       " 'BLUE': {'CAP',\n",
       "  'DANCE',\n",
       "  'ENGLAND',\n",
       "  'LASER',\n",
       "  'PILOT',\n",
       "  'PLATE',\n",
       "  'ROUND',\n",
       "  'SCORPION',\n",
       "  'SUIT'},\n",
       " 'BYSTANDER': {'AZTEC',\n",
       "  'KNIFE',\n",
       "  'LIGHT',\n",
       "  'LOCH NESS',\n",
       "  'ORGAN',\n",
       "  'SCREEN',\n",
       "  'SHOE'},\n",
       " 'RED': {'CHECK',\n",
       "  'CLOAK',\n",
       "  'COVER',\n",
       "  'CYCLE',\n",
       "  'ENGINE',\n",
       "  'EUROPE',\n",
       "  'IRON',\n",
       "  'WAVE'}}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.bag_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "d88f96a5-2ce5-4531-a45e-59a5cd5b5561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51 ms, sys: 4.44 ms, total: 55.4 ms\n",
      "Wall time: 4.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['WEARING', 'JACKET', 'UNIFORM', 'SHIRT', 'BLUE', 'WEAR', 'WORE',\n",
       "        'SIMILAR', 'PANTS', 'INSTEAD'], dtype='<U68'),\n",
       " array([0.34837234, 0.34546995, 0.33026087, 0.32659402, 0.30743918,\n",
       "        0.30555645, 0.30005243, 0.29982635, 0.29893532, 0.2983142 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "guesser.give_hint_candidates([\"cap\", \"suit\", \"pilot\"], strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "b0766093-d383-44e4-ad63-00f8ca15851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.43 ms, sys: 0 ns, total: 3.43 ms\n",
      "Wall time: 2.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['WEARING', 'JACKET', 'UNIFORM', 'SHIRT', 'BLUE', 'WEAR', 'WORE',\n",
       "        'SIMILAR', 'PANTS', 'INSTEAD'], dtype='<U68'),\n",
       " array([0.34837234, 0.34546995, 0.33026087, 0.326594  , 0.3074392 ,\n",
       "        0.30555642, 0.30005243, 0.29982635, 0.29893535, 0.2983142 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "guesser.give_hint_candidates([\"cap\", \"suit\", \"pilot\"], strategy=\"approx_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "1f7d65a3-eb0f-4ec4-9e52-a35571ed18f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 ms, sys: 0 ns, total: 2.56 ms\n",
      "Wall time: 2.21 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['DOFFING', 'HAT', 'GLENGARRY', 'AVIATOR', 'HEADGEAR', 'HELMET',\n",
       "        'VISOR', 'GARB', 'HEADDRESS', 'HATS'], dtype='<U19'),\n",
       " array([0.29665267, 0.295963  , 0.27957982, 0.27109832, 0.26435703,\n",
       "        0.2633745 , 0.25370625, 0.25083083, 0.24353588, 0.24231923],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cn_guesser.give_hint_candidates([\"cap\", \"suit\", \"pilot\"], strategy=\"approx_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1bb58920-e4cb-40d1-8407-6d7030bbb8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 ms, sys: 208 µs, total: 3.44 ms\n",
      "Wall time: 3.01 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['MOSELEY', 'GLOUCESTERSHIRE', 'YORKSHIRE', 'WARWICKSHIRE',\n",
       "        'BRISTOL', 'LEICESTERSHIRE', 'WILTSHIRE', 'MALDON',\n",
       "        'NORTHAMPTONSHIRE', 'NOTTINGHAM'], dtype='<U19'),\n",
       " array([0.24276574, 0.22583926, 0.22396934, 0.21759802, 0.21735172,\n",
       "        0.21629402, 0.21550824, 0.21187842, 0.20884211, 0.20858294],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cn_guesser.give_hint_candidates([\"england\", \"laser\", \"pilot\"], strategy=\"approx_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "faaeaae4-6cb4-4026-988a-4f3544d12254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['SUIT', 'CLOAK', 'CAP', 'COVER', 'SHOE', 'LIGHT', 'KNIFE', 'DANCE',\n",
       "        'IRON'], dtype='<U11'),\n",
       " array([0.5179965 , 0.40340713, 0.41855264, 0.2653496 , 0.35332826,\n",
       "        0.27642316, 0.25963306, 0.24954134, 0.16813308], dtype=float32))"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesser.guess(Hint(\"WEARING\", None, \"BLUE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "c7ba9797-8e8a-42d8-b441-02e43653e620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['CAP', 'SUIT', 'CLOAK', 'SHOE', 'PLATE', 'CYCLE', 'SCORPION'],\n",
       "       dtype='<U11'),\n",
       " array([0.29517464, 0.39653951, 0.37184003, 0.29065106, 0.05091697,\n",
       "        0.07256906, 0.05281432]))"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_guesser.guess(Hint(\"WEARING\", None, \"BLUE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "d7dc1740-4b8a-41ff-a308-43a55a77a9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.is_related_word(\"moles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "fdaa15f6-98e9-4c66-a37d-293ab22a24bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ROUND', 'CAP', 'SUIT', 'SCORPION', 'PILOT', 'LASER', 'ENGLAND',\n",
       "       'PLATE', 'DANCE'], dtype='<U11')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.blue_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5326641f-e466-4ef3-82dc-d63992b7aff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GloveGuesser.generate_word_suggestions_minimax of <__main__.GloveGuesser object at 0x7fdd4c289f40>>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guesser.strategy_lookup[\"minimax\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "786f8d45-7773-4436-b1bd-8ac23ed83c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_cosine_similarity(glove.vectors[:1], glove.vectors[:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4809fa12-f4c9-4c19-b22b-7d9de9215be2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[637,\n",
       " 1967,\n",
       " 325,\n",
       " 8334,\n",
       " 12038,\n",
       " 8427,\n",
       " 9214,\n",
       " 453,\n",
       " 5239,\n",
       " 12451,\n",
       " 3292,\n",
       " 2647,\n",
       " 12339,\n",
       " 603,\n",
       " 26900,\n",
       " 137,\n",
       " 1083,\n",
       " 775,\n",
       " 231,\n",
       " 2069,\n",
       " 14924,\n",
       " 4925,\n",
       " 5747,\n",
       " 1497,\n",
       " 3045,\n",
       " 960,\n",
       " 3827,\n",
       " 942,\n",
       " 2913,\n",
       " 5077,\n",
       " 2499,\n",
       " 11012,\n",
       " 9307,\n",
       " 480,\n",
       " 1963,\n",
       " 534,\n",
       " 11869,\n",
       " 1211,\n",
       " 1846,\n",
       " 4707,\n",
       " 10002,\n",
       " 6676,\n",
       " 7331,\n",
       " 1930,\n",
       " 1641,\n",
       " 9248,\n",
       " 8998,\n",
       " 4794,\n",
       " 11035,\n",
       " 41652,\n",
       " 6910,\n",
       " 14105,\n",
       " 774,\n",
       " 3539,\n",
       " 351,\n",
       " 569,\n",
       " 1904,\n",
       " 20786,\n",
       " 5391,\n",
       " 1784,\n",
       " 5450,\n",
       " 2114,\n",
       " 46995,\n",
       " 313,\n",
       " 3845,\n",
       " 511,\n",
       " 1090,\n",
       " 2375,\n",
       " 5778,\n",
       " 17489,\n",
       " 132,\n",
       " 6242,\n",
       " 512,\n",
       " 4012,\n",
       " 8525,\n",
       " 23755,\n",
       " 449,\n",
       " 2280,\n",
       " 1866,\n",
       " 4249,\n",
       " 3990,\n",
       " 3031,\n",
       " 8475,\n",
       " 953,\n",
       " 3387,\n",
       " 5139,\n",
       " 5223,\n",
       " 202,\n",
       " 1333,\n",
       " 10216,\n",
       " 2005,\n",
       " 2162,\n",
       " 1007,\n",
       " 3120,\n",
       " 4124,\n",
       " 2312,\n",
       " 2261,\n",
       " 1257,\n",
       " 122,\n",
       " 336,\n",
       " 6744,\n",
       " 1714,\n",
       " 5188,\n",
       " 14621,\n",
       " 13308,\n",
       " 1289,\n",
       " 2082,\n",
       " 2926,\n",
       " 1737,\n",
       " 7394,\n",
       " 4635,\n",
       " 8416,\n",
       " 1560,\n",
       " 7774,\n",
       " 15389,\n",
       " 5838,\n",
       " 1598,\n",
       " 1847,\n",
       " 2100,\n",
       " 563,\n",
       " 525,\n",
       " 2090,\n",
       " 621,\n",
       " 1791,\n",
       " 807,\n",
       " 3267,\n",
       " 6043,\n",
       " 307263,\n",
       " 3510,\n",
       " 1265,\n",
       " 2854,\n",
       " 319,\n",
       " 484,\n",
       " 2120,\n",
       " 16677,\n",
       " 2361,\n",
       " 2149,\n",
       " 352,\n",
       " 2061,\n",
       " 11532,\n",
       " 387,\n",
       " 186,\n",
       " 851,\n",
       " 9780,\n",
       " 509,\n",
       " 7431,\n",
       " 1752,\n",
       " 2847,\n",
       " 10922,\n",
       " 764,\n",
       " 5049,\n",
       " 4614,\n",
       " 2107,\n",
       " 989,\n",
       " 817,\n",
       " 7174,\n",
       " 823,\n",
       " 9993,\n",
       " 362,\n",
       " 1058,\n",
       " 3134,\n",
       " 22574,\n",
       " 2924,\n",
       " 2290,\n",
       " 9812,\n",
       " 7721,\n",
       " 7913,\n",
       " 6832,\n",
       " 2867,\n",
       " 25145,\n",
       " 726,\n",
       " 1228,\n",
       " 1718,\n",
       " 1718,\n",
       " 5161,\n",
       " 474,\n",
       " 2906,\n",
       " 5117,\n",
       " 2180,\n",
       " 8202,\n",
       " 3423,\n",
       " 11738,\n",
       " 22184,\n",
       " 31156,\n",
       " 638,\n",
       " 4313,\n",
       " 691,\n",
       " 22500,\n",
       " 7209,\n",
       " 4848,\n",
       " 5577,\n",
       " 5452,\n",
       " 8013,\n",
       " 1387,\n",
       " 410,\n",
       " 9311,\n",
       " 78868,\n",
       " 214,\n",
       " 897,\n",
       " 18634,\n",
       " 331,\n",
       " 2858,\n",
       " 6657,\n",
       " 18990,\n",
       " 18045,\n",
       " 24160,\n",
       " 6296,\n",
       " 9000,\n",
       " 516,\n",
       " 5556,\n",
       " 1339,\n",
       " 14420,\n",
       " 9765,\n",
       " 9342,\n",
       " 304,\n",
       " 1329,\n",
       " 550,\n",
       " 6903,\n",
       " 856,\n",
       " 21519,\n",
       " 10233,\n",
       " 2398,\n",
       " 12165,\n",
       " 2079,\n",
       " 1423,\n",
       " 20026,\n",
       " 3377,\n",
       " 1341,\n",
       " 2787,\n",
       " 7571,\n",
       " 4166,\n",
       " 26869,\n",
       " 12109,\n",
       " 14164,\n",
       " 1050,\n",
       " 50,\n",
       " 196,\n",
       " 364,\n",
       " 22107,\n",
       " 1812,\n",
       " 1999,\n",
       " 6794,\n",
       " 16849,\n",
       " 27547,\n",
       " 316,\n",
       " 6952,\n",
       " 29640,\n",
       " 2832,\n",
       " 3200,\n",
       " 6480,\n",
       " 3214,\n",
       " 3670,\n",
       " 8176,\n",
       " 1307,\n",
       " 13992,\n",
       " 625,\n",
       " 153,\n",
       " 1280,\n",
       " 17003,\n",
       " 15534,\n",
       " 3608,\n",
       " 4809,\n",
       " 9246,\n",
       " 2498,\n",
       " 8426,\n",
       " 7643,\n",
       " 10881,\n",
       " 9459,\n",
       " 6576,\n",
       " 3098,\n",
       " 1313,\n",
       " 3754,\n",
       " 4364,\n",
       " 68860,\n",
       " 282,\n",
       " 2219,\n",
       " 389,\n",
       " 9373,\n",
       " 4924,\n",
       " 142,\n",
       " 3216,\n",
       " 1414,\n",
       " 658,\n",
       " 3605,\n",
       " 428,\n",
       " 4242,\n",
       " 17943,\n",
       " 12898,\n",
       " 11673,\n",
       " 2060,\n",
       " 12425,\n",
       " 17676,\n",
       " 3014,\n",
       " 2525,\n",
       " 2930,\n",
       " 4972,\n",
       " 9247,\n",
       " 1137,\n",
       " 2618,\n",
       " 5440,\n",
       " 486,\n",
       " 33476,\n",
       " 469,\n",
       " 2307,\n",
       " 7154,\n",
       " 2805,\n",
       " 12609,\n",
       " 2121,\n",
       " 164,\n",
       " 4756,\n",
       " 27940,\n",
       " 2491,\n",
       " 23603,\n",
       " 18849,\n",
       " 5874,\n",
       " 8069,\n",
       " 5675,\n",
       " 7477,\n",
       " 9861,\n",
       " 1370,\n",
       " 8597,\n",
       " 2855,\n",
       " 635,\n",
       " 9241,\n",
       " 19634,\n",
       " 6201,\n",
       " 13889,\n",
       " 27898,\n",
       " 2642,\n",
       " 48146,\n",
       " 27696,\n",
       " 2481,\n",
       " 4184,\n",
       " 1507,\n",
       " 863,\n",
       " 6252,\n",
       " 11539,\n",
       " 8789,\n",
       " 15252,\n",
       " 1845,\n",
       " 1509,\n",
       " 5028,\n",
       " 1249,\n",
       " 1352,\n",
       " 821,\n",
       " 753,\n",
       " 92,\n",
       " 4403,\n",
       " 452,\n",
       " 7397,\n",
       " 5220,\n",
       " 1134,\n",
       " 3758,\n",
       " 5520,\n",
       " 2536,\n",
       " 18561,\n",
       " 4923,\n",
       " 5022,\n",
       " 1801,\n",
       " 18493,\n",
       " 6647,\n",
       " 6780,\n",
       " 7053,\n",
       " 2513,\n",
       " 10944,\n",
       " 2610,\n",
       " 2248,\n",
       " 14402,\n",
       " 10686,\n",
       " 20571,\n",
       " 2668,\n",
       " 79,\n",
       " 1363,\n",
       " 13326,\n",
       " 8638,\n",
       " 2935,\n",
       " 1008,\n",
       " 1470,\n",
       " 9464,\n",
       " 1305,\n",
       " 10032,\n",
       " 7370,\n",
       " 1130,\n",
       " 36538,\n",
       " 36200,\n",
       " 9316,\n",
       " 1461,\n",
       " 22387,\n",
       " 3050,\n",
       " 1015,\n",
       " 136,\n",
       " 38479,\n",
       " 289,\n",
       " 1716,\n",
       " 430,\n",
       " 2686,\n",
       " 1170,\n",
       " 143,\n",
       " 9287,\n",
       " 9933,\n",
       " 2486,\n",
       " 10825,\n",
       " 16528,\n",
       " 4758]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.tokenize(\" \".join(wordlist.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "003eb92d-0b04-4e2b-adee-1a75061f1b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04656  ,  0.21318  , -0.0074364, ...,  0.0090611, -0.20989  ,\n",
       "         0.053913 ],\n",
       "       [-0.25539  , -0.25723  ,  0.13169  , ..., -0.2329   , -0.12226  ,\n",
       "         0.35499  ],\n",
       "       [-0.12559  ,  0.01363  ,  0.10306  , ..., -0.34224  , -0.022394 ,\n",
       "         0.13684  ],\n",
       "       ...,\n",
       "       [ 0.075713 , -0.040502 ,  0.18345  , ...,  0.21838  ,  0.30967  ,\n",
       "         0.43761  ],\n",
       "       [ 0.81451  , -0.36221  ,  0.31186  , ...,  0.075486 ,  0.28408  ,\n",
       "        -0.17559  ],\n",
       "       [ 0.429191 , -0.296897 ,  0.15011  , ...,  0.28975  ,  0.32618  ,\n",
       "        -0.0590532]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115bc2c-caba-49e1-bdfd-71f85f766686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (fn_env)",
   "language": "python",
   "name": "fn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
